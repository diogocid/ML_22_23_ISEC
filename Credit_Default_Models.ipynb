{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utilities\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import skewtest\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def romanToInt(i):   \n",
    "    roman = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000,'IV':4,'IX':9,'XL':40,'XC':90,'CD':400,'CM':900}\n",
    "    j = 0\n",
    "    num = 0\n",
    "    while j < len(i):\n",
    "        if j+1<len(i) and i[j:j+2] in roman:\n",
    "            num+=roman[i[j:j+2]]\n",
    "            j+=2\n",
    "        else:\n",
    "\n",
    "            num+=roman[i[j]]\n",
    "            j+=1\n",
    "    return num\n",
    "\n",
    "def skew_df(df):\n",
    "    skewness, p_value = skewtest(df)\n",
    "    dskew=pd.DataFrame(np.round(np.vstack((skewness.T,p_value.T)),2),columns=df.columns,\n",
    "                    index=['skewness', 'p_value'])\n",
    "    return(dskew)\n",
    "\n",
    "def plot_roc_curve(fpr,tpr):\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.plot([0,1],[0,1],'r')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC - TPR vs FPR')\n",
    "    \n",
    "def printCustomMetrics(y_test, y_pred):\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred).round(2))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred).round(2))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred).round(2))\n",
    "    print(\"f1:\", metrics.f1_score(y_test, y_pred).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87500, 30)\n"
     ]
    }
   ],
   "source": [
    "#Import Dataset\n",
    "df = pd.read_csv('loan_default_prediction.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87500, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop\n",
    "vdrop=['ID','Validation','Designation','Debt_to_Income','Postal_Code','Deprecatory_Records',\\\n",
    "            'Inquiries','Gross_Collection','Sub_GGGrade','Total_Unpaid_CL','File_Status','Claim_Type','Due_Fee']\n",
    "df=df.drop(vdrop,axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversão dos anos de experiência para numérico\n",
    "df['Experience']=df['Experience'].apply(lambda i: 10 if i[0:1]=='>' else 1 if i[0:1]=='<' else int(i[0:1]))\n",
    "#Conversão da duração para numérico\n",
    "df['Duration']=df['Duration'].apply(lambda i : i.replace(' years','years')).astype(str)\n",
    "#Conversão da GGGrade valor ordinal para numérico\n",
    "df['GGGrade']=df['GGGrade'].apply(romanToInt).astype(int)\n",
    "#criacao de debt to income\n",
    "#calcular o total da divida e o rendimento anual. uma espécie de \"taxa de esforço\"\n",
    "df['debt_to_income']=df['Unpaid_Amount']/df['Yearly_Income']\n",
    "#ver resultado\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77376, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminar observações com pelo menos uma feature sem valores\n",
    "df=df.dropna()\n",
    "#drop duplicates\n",
    "df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asst_Reg</th>\n",
       "      <th>GGGrade</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Yearly_Income</th>\n",
       "      <th>Home_Status</th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>Lend_Amount</th>\n",
       "      <th>Interest_Charged</th>\n",
       "      <th>Usage_Rate</th>\n",
       "      <th>Present_Balance</th>\n",
       "      <th>State</th>\n",
       "      <th>Account_Open</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Unpaid_Amount</th>\n",
       "      <th>Reason</th>\n",
       "      <th>Default</th>\n",
       "      <th>debt_to_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>421802</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>633600.00</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42023.25</td>\n",
       "      <td>15.39</td>\n",
       "      <td>88.924</td>\n",
       "      <td>607161.90</td>\n",
       "      <td>California</td>\n",
       "      <td>17</td>\n",
       "      <td>3years</td>\n",
       "      <td>31216.05</td>\n",
       "      <td>debt  consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3964312</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>85483.20</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38133.00</td>\n",
       "      <td>9.94</td>\n",
       "      <td>102.856</td>\n",
       "      <td>269234.06</td>\n",
       "      <td>NC</td>\n",
       "      <td>15</td>\n",
       "      <td>5years</td>\n",
       "      <td>11660.49</td>\n",
       "      <td>debt  consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4247560</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>79200.00</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17100.00</td>\n",
       "      <td>22.35</td>\n",
       "      <td>60.372</td>\n",
       "      <td>22476.53</td>\n",
       "      <td>Florida</td>\n",
       "      <td>7</td>\n",
       "      <td>5years</td>\n",
       "      <td>5637.87</td>\n",
       "      <td>major  purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197179</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>61600.00</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5130.00</td>\n",
       "      <td>10.36</td>\n",
       "      <td>116.272</td>\n",
       "      <td>15242.09</td>\n",
       "      <td>NewJersey</td>\n",
       "      <td>9</td>\n",
       "      <td>3years</td>\n",
       "      <td>15607.17</td>\n",
       "      <td>major  purchase</td>\n",
       "      <td>1</td>\n",
       "      <td>0.253363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4646684</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>68053.92</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19665.00</td>\n",
       "      <td>13.68</td>\n",
       "      <td>127.280</td>\n",
       "      <td>65433.94</td>\n",
       "      <td>LA</td>\n",
       "      <td>10</td>\n",
       "      <td>5years</td>\n",
       "      <td>27472.86</td>\n",
       "      <td>debt  consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Asst_Reg  GGGrade  Experience  Yearly_Income Home_Status  Unpaid_2_years  \\\n",
       "0    421802        2          10      633600.00    MORTGAGE               0   \n",
       "1   3964312        4           7       85483.20        RENT               0   \n",
       "2   4247560        3           1       79200.00        RENT               0   \n",
       "3    197179        3           1       61600.00        RENT               0   \n",
       "4   4646684        5           2       68053.92        RENT               0   \n",
       "\n",
       "   Already_Defaulted  Lend_Amount  Interest_Charged  Usage_Rate  \\\n",
       "0                  0     42023.25             15.39      88.924   \n",
       "1                  0     38133.00              9.94     102.856   \n",
       "2                  0     17100.00             22.35      60.372   \n",
       "3                  0      5130.00             10.36     116.272   \n",
       "4                  0     19665.00             13.68     127.280   \n",
       "\n",
       "   Present_Balance       State  Account_Open Duration  Unpaid_Amount  \\\n",
       "0        607161.90  California            17   3years       31216.05   \n",
       "1        269234.06          NC            15   5years       11660.49   \n",
       "2         22476.53     Florida             7   5years        5637.87   \n",
       "3         15242.09   NewJersey             9   3years       15607.17   \n",
       "4         65433.94          LA            10   5years       27472.86   \n",
       "\n",
       "                Reason  Default  debt_to_income  \n",
       "0  debt  consolidation        0        0.049268  \n",
       "1  debt  consolidation        0        0.136407  \n",
       "2      major  purchase        0        0.071185  \n",
       "3      major  purchase        1        0.253363  \n",
       "4  debt  consolidation        0        0.403693  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.describe()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_num_cont=['Asst_Reg','Experience','Yearly_Income','Lend_Amount','Interest_Charged','Usage_Rate',\n",
    "            'Present_Balance','Unpaid_Amount','debt_to_income']\n",
    "v_num_disc=['Unpaid_2_years','Already_Defaulted','Account_Open']\n",
    "v_cat_ord=['Home_Status','State','Reason','Duration']#,'GGGrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77376, 18)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(74542, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "display(df.shape)\n",
    "#remover observações com home status 'none' e 'other'\n",
    "df=df[(df['Home_Status']!='OTHER')&(df['Home_Status']!='NONE')]\n",
    "#remover linhas com outliers, definidos como mais que 3 desvios-padrão acima/abaixo da média\n",
    "df=df[(np.abs(stats.zscore(df[v_num_cont])) < 3).all(axis=1)]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ver quantas observações têm default=1\n",
    "display((df['Default']==1).sum())\n",
    "#sample de 5000 obs com default =1\n",
    "defaulted = df[df['Default']==1].sample(n=5000, random_state=101)\n",
    "#sample de 5000 obs com default =0\n",
    "notdefault = df[df['Default']==0].sample(n=5000, random_state=101)\n",
    "#agregar as observações\n",
    "df = pd.concat([defaulted,notdefault],axis=0)\n",
    "#sort aleatório das obs\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>Account_Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>12.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.855792</td>\n",
       "      <td>0.076632</td>\n",
       "      <td>5.129104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unpaid_2_years  Already_Defaulted  Account_Open\n",
       "count    10000.000000       10000.000000  10000.000000\n",
       "mean         0.315900           0.005300     12.526900\n",
       "std          0.855792           0.076632      5.129104\n",
       "min          0.000000           0.000000      2.000000\n",
       "25%          0.000000           0.000000      9.000000\n",
       "50%          0.000000           0.000000     12.000000\n",
       "75%          0.000000           0.000000     15.000000\n",
       "max         16.000000           2.000000     51.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[v_num_disc].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = df[v_num_cont+v_num_disc+v_cat_ord]\n",
    "y = df['Default']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines: transformação de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asst_Reg</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Yearly_Income</th>\n",
       "      <th>Lend_Amount</th>\n",
       "      <th>Interest_Charged</th>\n",
       "      <th>Usage_Rate</th>\n",
       "      <th>Present_Balance</th>\n",
       "      <th>Unpaid_Amount</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>Account_Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>skewness</th>\n",
       "      <td>13.72</td>\n",
       "      <td>-5.77</td>\n",
       "      <td>38.53</td>\n",
       "      <td>23.17</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-5.86</td>\n",
       "      <td>35.08</td>\n",
       "      <td>42.59</td>\n",
       "      <td>33.6</td>\n",
       "      <td>76.21</td>\n",
       "      <td>108.02</td>\n",
       "      <td>36.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_value</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Asst_Reg  Experience  Yearly_Income  Lend_Amount  Interest_Charged  \\\n",
       "skewness     13.72       -5.77          38.53        23.17             -0.45   \n",
       "p_value       0.00        0.00           0.00         0.00              0.65   \n",
       "\n",
       "          Usage_Rate  Present_Balance  Unpaid_Amount  debt_to_income  \\\n",
       "skewness       -5.86            35.08          42.59            33.6   \n",
       "p_value         0.00             0.00           0.00             0.0   \n",
       "\n",
       "          Unpaid_2_years  Already_Defaulted  Account_Open  \n",
       "skewness           76.21             108.02         36.18  \n",
       "p_value             0.00               0.00          0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Asst_Reg',\n",
       " 'Experience',\n",
       " 'Yearly_Income',\n",
       " 'Lend_Amount',\n",
       " 'Usage_Rate',\n",
       " 'Present_Balance',\n",
       " 'Unpaid_Amount',\n",
       " 'debt_to_income',\n",
       " 'Unpaid_2_years',\n",
       " 'Already_Defaulted',\n",
       " 'Account_Open']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificar assimetria das features numéricas\n",
    "dskew=skew_df(X_train[v_num_cont+v_num_disc])\n",
    "#mostrar output\n",
    "display(dskew)\n",
    "#ver lista de features assimétricas, pvalue < 5%\n",
    "v_skew=list(dskew.columns[dskew.loc['p_value']<0.05])\n",
    "#lista de simétricas são as restantes\n",
    "v_sym=list(set(X_train[v_num_cont+v_num_disc].columns) - set(v_skew))\n",
    "#check\n",
    "v_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features com predominância em mais de 5% das observações:['MORTGAGE', 'RENT', 'OWN']\n",
      "Features com predominância em menos de 5% das observações:[]\n",
      "Features com predominância em mais de 5% das observações:['California', 'TX', 'Newyork', 'Florida']\n",
      "Features com predominância em menos de 5% das observações:['IL', 'NewJersey', 'PA', 'GA', 'Ohio', 'NC', 'MI', 'VA', 'Maryland', 'AZ', 'CO', 'WA', 'MA', 'MN', 'TN', 'MO', 'NV', 'IN', 'OR', 'SC', 'AL', 'WI', 'CT', 'LA', 'KS', 'AR', 'OK', 'KY', 'UT', 'HI', 'RI', 'NM', 'WV', 'DC', 'NH', 'MS', 'MT', 'AK', 'DE', 'WY', 'SD', 'NE', 'VT', 'ND', 'ME']\n",
      "Features com predominância em mais de 5% das observações:['debt  consolidation', 'credit  card', 'home  improvement']\n",
      "Features com predominância em menos de 5% das observações:['other', 'major  purchase', 'medical', 'car', 'small  business', 'moving', 'vacation', 'house', 'wedding', 'RENTwable  energy']\n",
      "Features com predominância em mais de 5% das observações:['3years', '5years']\n",
      "Features com predominância em menos de 5% das observações:[]\n"
     ]
    }
   ],
   "source": [
    "for i in v_cat_ord:\n",
    "    c=df[i].value_counts()/df.shape[0]\n",
    "    #list(c[c>.05].index)\n",
    "    print('Features com predominância em mais de 5% das observações:'+str(list(c[c>=.05].index)))\n",
    "    print('Features com predominância em menos de 5% das observações:'+str(list(c[c<.05].index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asst_Reg</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Yearly_Income</th>\n",
       "      <th>Lend_Amount</th>\n",
       "      <th>Usage_Rate</th>\n",
       "      <th>Present_Balance</th>\n",
       "      <th>Unpaid_Amount</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>...</th>\n",
       "      <th>Florida</th>\n",
       "      <th>Newyork</th>\n",
       "      <th>TX</th>\n",
       "      <th>Other_Cat1</th>\n",
       "      <th>credit  card</th>\n",
       "      <th>debt  consolidation</th>\n",
       "      <th>home  improvement</th>\n",
       "      <th>Other_Cat2</th>\n",
       "      <th>3years</th>\n",
       "      <th>5years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "      <td>8000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.27</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>-2.64</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.83</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Asst_Reg  Experience  Yearly_Income  Lend_Amount  Usage_Rate  \\\n",
       "count   8000.00     8000.00        8000.00      8000.00     8000.00   \n",
       "mean       0.00        0.00          -0.00         0.00        0.00   \n",
       "std        1.00        1.00           1.00         1.00        1.00   \n",
       "min       -2.27       -1.52          -3.99        -2.64       -2.37   \n",
       "25%       -0.86       -0.83          -0.68        -0.74       -0.72   \n",
       "50%       -0.00        0.05           0.02        -0.03        0.05   \n",
       "75%        0.93        1.08           0.69         0.77        0.76   \n",
       "max        1.48        1.08           2.83         1.99        2.88   \n",
       "\n",
       "       Present_Balance  Unpaid_Amount  debt_to_income  Unpaid_2_years  \\\n",
       "count          8000.00        8000.00         8000.00         8000.00   \n",
       "mean             -0.00           0.00            0.00            0.00   \n",
       "std               1.00           1.00            1.00            1.00   \n",
       "min              -3.90          -3.46           -2.09           -0.48   \n",
       "25%              -0.80          -0.67           -0.76           -0.48   \n",
       "50%              -0.06          -0.03           -0.01           -0.48   \n",
       "75%               0.85           0.67            0.74           -0.48   \n",
       "max               2.08           3.21            2.41            2.10   \n",
       "\n",
       "       Already_Defaulted  ...  Florida  Newyork       TX  Other_Cat1  \\\n",
       "count            8000.00  ...  8000.00  8000.00  8000.00     8000.00   \n",
       "mean                0.00  ...     0.06     0.08     0.08        0.62   \n",
       "std                 1.00  ...     0.25     0.27     0.27        0.48   \n",
       "min                -0.07  ...     0.00     0.00     0.00        0.00   \n",
       "25%                -0.07  ...     0.00     0.00     0.00        0.00   \n",
       "50%                -0.07  ...     0.00     0.00     0.00        1.00   \n",
       "75%                -0.07  ...     0.00     0.00     0.00        1.00   \n",
       "max                13.93  ...     1.00     1.00     1.00        1.00   \n",
       "\n",
       "       credit  card  debt  consolidation  home  improvement  Other_Cat2  \\\n",
       "count       8000.00              8000.00            8000.00     8000.00   \n",
       "mean           0.24                 0.60               0.05        0.11   \n",
       "std            0.43                 0.49               0.22        0.31   \n",
       "min            0.00                 0.00               0.00        0.00   \n",
       "25%            0.00                 0.00               0.00        0.00   \n",
       "50%            0.00                 1.00               0.00        0.00   \n",
       "75%            0.00                 1.00               0.00        0.00   \n",
       "max            1.00                 1.00               1.00        1.00   \n",
       "\n",
       "        3years   5years  \n",
       "count  8000.00  8000.00  \n",
       "mean      0.72     0.28  \n",
       "std       0.45     0.45  \n",
       "min       0.00     0.00  \n",
       "25%       0.00     0.00  \n",
       "50%       1.00     0.00  \n",
       "75%       1.00     1.00  \n",
       "max       1.00     1.00  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asst_Reg</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Yearly_Income</th>\n",
       "      <th>Lend_Amount</th>\n",
       "      <th>Usage_Rate</th>\n",
       "      <th>Present_Balance</th>\n",
       "      <th>Unpaid_Amount</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>Account_Open</th>\n",
       "      <th>Interest_Charged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>skewness</th>\n",
       "      <td>-5.85</td>\n",
       "      <td>-9.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-6.95</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.19</td>\n",
       "      <td>105.32</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_value</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Asst_Reg  Experience  Yearly_Income  Lend_Amount  Usage_Rate  \\\n",
       "skewness     -5.85       -9.02            0.0        -1.61       -6.95   \n",
       "p_value       0.00        0.00            1.0         0.11        0.00   \n",
       "\n",
       "          Present_Balance  Unpaid_Amount  debt_to_income  Unpaid_2_years  \\\n",
       "skewness            -2.19           1.09             3.0           42.19   \n",
       "p_value              0.03           0.28             0.0            0.00   \n",
       "\n",
       "          Already_Defaulted  Account_Open  Interest_Charged  \n",
       "skewness             105.32          -0.0             -0.45  \n",
       "p_value                0.00           1.0              0.65  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer, FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the transformations to apply to the column\n",
    "transformer = ColumnTransformer([\n",
    "    ('yeoj', PowerTransformer(), v_skew), #aplico transformação que corrige assimetria às assimétricas\n",
    "    ('std', StandardScaler(), v_sym),     #aplico transformação às simétricas (sub média e dividir desvio padrao)\n",
    "    ('oneh', OneHotEncoder(min_frequency=0.05,handle_unknown='ignore', sparse_output=False), v_cat_ord)\n",
    "])\n",
    "\n",
    "# Transform the data\n",
    "pfit = transformer.fit(X_train)\n",
    "\n",
    "#Create dataframe with transformation\n",
    "categories= pfit.transformers_[2][1].categories_\n",
    "categories_out=pfit.transformers_[2][1].infrequent_categories_\n",
    "\n",
    "v_onehot_drop=list(np.concatenate([np.concatenate([categories[i][np.isin(categories[i], categories_out[i], invert=True)]],axis=0) \n",
    "                        for i in range(0,len(categories))],axis=0))\n",
    "\n",
    "\n",
    "v_onehot=list(np.concatenate([(np.concatenate((np.array(j),\n",
    "                       (np.array(['Other_Cat'+str(k)]) if categories_out[k] is not None else np.array([]))),axis=0)) \n",
    "                         for k,j in enumerate(\n",
    "                         [np.concatenate([categories[i][np.isin(categories[i], categories_out[i], invert=True)]],axis=0) \n",
    "                        for i in range(0,len(categories))]\n",
    "                         )]))\n",
    "\n",
    "#Transform train and test X\n",
    "X_train_transf = pd.DataFrame(pfit.transform(X_train),columns = (v_skew + v_sym+v_onehot)\n",
    "                              ,index=X_train.index)\n",
    "\n",
    "X_test_transf = pd.DataFrame(pfit.transform(X_test),columns = (v_skew + v_sym+v_onehot)\n",
    "                            ,index=X_test.index)\n",
    "\n",
    "display(round(X_train_transf.describe(),2))\n",
    "\n",
    "#vamos dar um check se o dataframe ficou \"menos assimétrico\"\n",
    "skew_df(X_train_transf[v_skew+v_sym])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não usar bernoulli porque faz mais sentido para variaveis binarias\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "rnd_clf=RandomForestClassifier(n_estimators=100)\n",
    "log_clf=LogisticRegression()\n",
    "svm_clf=SVC()\n",
    "dtc_clf=DecisionTreeClassifier()\n",
    "knn_clf=KNeighborsClassifier()\n",
    "mpl_clf=MLPClassifier()\n",
    "gnb_clf=GaussianNB()\n",
    "#Adicionar o resto dos modelos superviselearning menos o xgboost\n",
    "#bnb_clf=BernoulliNB()\n",
    "\n",
    "\n",
    "voting_clf=VotingClassifier(\n",
    "    estimators=[('lr',log_clf),('rf',rnd_clf),('svm',svm_clf)\n",
    "                ,('dtc',dtc_clf),('knn',knn_clf),('mpl',mpl_clf),('gnb',gnb_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    " \n",
    "dataMetrics = pd.DataFrame(columns=['Accuracy','Precision','Recall','F1'])\n",
    "for i, clf in enumerate([log_clf,rnd_clf,svm_clf,dtc_clf,knn_clf,mpl_clf,gnb_clf,voting_clf], start=1):\n",
    "    clf.fit(X_train_transf,y_train)\n",
    "    y_pred=clf.predict(X_test_transf)   \n",
    "    #DataFrame\n",
    "    dataMetrics.loc[clf.__class__.__name__, ['Accuracy']] = metrics.accuracy_score(y_test, y_pred).round(2)\n",
    "    dataMetrics.loc[clf.__class__.__name__, ['Precision']] = metrics.precision_score(y_test, y_pred).round(2)\n",
    "    dataMetrics.loc[clf.__class__.__name__, ['Recall']] = metrics.recall_score(y_test, y_pred).round(2)\n",
    "    dataMetrics.loc[clf.__class__.__name__, ['F1']] = metrics.f1_score(y_test, y_pred).round(2)\n",
    "    #Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    if i % 2 == 0:\n",
    "        sns.heatmap(cf_matrix, annot=True, fmt='g');\n",
    "    else:\n",
    "        sns.heatmap(cf_matrix, annot=True, fmt='g', cmap='Blues');\n",
    "    plt.show()\n",
    "    #Generate the ROC AUC Curve\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    #ROC AUC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,y_pred)\n",
    "    plot_roc_curve(fpr,tpr)\n",
    "    plt.show()\n",
    "    \n",
    "dataMetrics\n",
    "    \n",
    "#experimentar pesos, fazer grid search\n",
    "\n",
    "#APLICAR TRAIN TEST UMAS 30 VEZES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar script dos classificador com hiperparametros\n",
    "rnd_clf_best_params={'bootstrap': True,\n",
    "                     'max_depth': 8,\n",
    "                     'max_features': 'sqrt',\n",
    "                    'min_samples_leaf': 6,\n",
    "                     'min_samples_split': 20,\n",
    "                     'n_estimators': 100}\n",
    "log_clf_best_params={'C': 1}\n",
    "svm_clf_best_params={'C': 1, 'kernel': 'rbf'}\n",
    "dtc_clf_best_params={'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 6, 'min_samples_split': 10}\n",
    "knn_clf_best_params={'leaf_size': 10,\n",
    "                     'metric': 'euclidean',\n",
    "                     'n_neighbors': 9,\n",
    "                     'weights': 'distance'}\n",
    "mpl_clf_best_params={'activation': 'logistic', 'hidden_layer_sizes': (8,), 'solver': 'adam'}\n",
    "gnb_clf_best_params={}\n",
    "#xgboost\n",
    "xgb_clf_best_params={'gamma': 0,\n",
    "                     'learning_rate': 0.01,\n",
    "                     'max_depth': 3,\n",
    "                     'n_estimators': 100,\n",
    "                     'reg_alpha': 0.5,\n",
    "                     'reg_lambda': 1,\n",
    "                     'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf=RandomForestClassifier(**rnd_clf_best_params)\n",
    "log_clf=LogisticRegression(**log_clf_best_params)\n",
    "svm_clf=SVC(**svm_clf_best_params)\n",
    "dtc_clf=DecisionTreeClassifier(**dtc_clf_best_params)\n",
    "knn_clf=KNeighborsClassifier(**knn_clf_best_params)\n",
    "mpl_clf=MLPClassifier(**mpl_clf_best_params)\n",
    "gnb_clf=GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply PCA to the training data to reduce the dimensionality\n",
    "#ALWAYS STANDARDIZE\n",
    "#VER SE VALE A PENA FAZER UNS GRAFICOS\n",
    "#POR EXEMPLO\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=0.9999)\n",
    "\n",
    "X_pca=pca.fit(X_train_transf[v_skew+v_sym])\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "#\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "#\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "#\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "#\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Use cross-validation to evaluate logistic regression with different numbers of principal components\n",
    "pca = PCA()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "n_components = range(1, X_train_transf[v_skew+v_sym].shape[1]+1)\n",
    "scores = []\n",
    "display(X_train_transf[v_skew+v_sym].shape[1])\n",
    "\n",
    "for n in n_components:\n",
    "    pca.n_components = n\n",
    "    X_train_pca = pca.fit_transform(X_train_transf[v_skew+v_sym])\n",
    "    score = np.mean(cross_val_score(logreg, X_train_pca, y_train, cv=5, scoring='f1'))\n",
    "    scores.append(score)\n",
    "\n",
    "# Select the number of components that gives the highest cross-validation score\n",
    "optimal_n_components = n_components[np.argmax(scores)]\n",
    "display(scores)\n",
    "#display(n_components)\n",
    "\n",
    "# Train logistic regression model on full training set with optimal number of components\n",
    "pca.n_components = optimal_n_components\n",
    "display(optimal_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=optimal_n_components)\n",
    "\n",
    "X_pca=pca.fit(X_train_transf[v_skew+v_sym])\n",
    "\n",
    "X_train_num_pca=pd.DataFrame(pca.transform(X_train_transf[v_skew+v_sym]),\n",
    "                         columns=['pca_v'+str(i+1) for i in range (0,X_pca.n_components_)],\n",
    "                        index=X_train_transf.index)\n",
    "\n",
    "X_test_num_pca=pd.DataFrame(pca.transform(X_test_transf[v_skew+v_sym]),\n",
    "                         columns=['pca_v'+str(i+1) for i in range (0,X_pca.n_components_)],\n",
    "                        index=X_test_transf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_pca=pd.concat([X_train_num_pca,pd.DataFrame(y_train)],axis=1)\n",
    "#display(df_pca.head(5))\n",
    "\n",
    "sns.scatterplot(x='pca_v1', y='pca_v2', hue='Default', data=df_pca);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='pca_v6', y='pca_v7', hue='Default', data=df_pca);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_pca.corr()\n",
    "display(round(corr.iloc[[-1],:],2))\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,cmap=\"BuPu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar X com PCA\n",
    "X_train_pca=pd.concat(\n",
    "    [X_train_num_pca,\n",
    "        X_train_transf[X_train_transf.columns[-(len(X_train_transf.columns)-len(v_skew+v_sym)):]]],\n",
    "     axis=1)\n",
    "\n",
    "X_test_pca=pd.concat(\n",
    "    [X_test_num_pca,\n",
    "        X_test_transf[X_test_transf.columns[-(len(X_test_transf.columns)-len(v_skew+v_sym)):]]],\n",
    "     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse=[]\n",
    "silhscores = []\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "k_rng=range(1,15)\n",
    "for k in k_rng:\n",
    "    km=KMeans(n_clusters=k,n_init=10)\n",
    "    km.fit(X_train_transf)\n",
    "    sse.append(km.inertia_)\n",
    "    if k>1:\n",
    "        km_pred=km.predict(X_train_transf)\n",
    "        silhscore = silhouette_score(X_train_transf, km_pred)\n",
    "        silhscores.append(silhscore)\n",
    "display(silhscores)    \n",
    "sse\n",
    "\n",
    "#--Lento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('K')\n",
    "plt.ylabel('Sum of squared error')\n",
    "plt.xticks(k_rng)\n",
    "plt.plot(k_rng,sse, '-o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the silhouette scores\n",
    "plt.plot(range(2, 15), silhscores, '-o')\n",
    "plt.xticks(range(2, 15))\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters identified by silhouette\n",
    "k_silh=range(2,15)[np.array(silhscores).argmax()]\n",
    "display('The silhouette score has chosen '+str(k_silh)+' clusters.')\n",
    "km=KMeans(n_clusters=k_silh,n_init=10)\n",
    "km_fit=km.fit(X_train_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Centros dos clusters\n",
    "cluster=km_fit.predict(X_train_transf)\n",
    "unique, counts = np.unique(cluster, return_counts=True)\n",
    "display(pd.DataFrame(np.asarray((unique, counts)).T,columns=['cluster','no of obs']))\n",
    "\n",
    "#km_fit.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "ohe=OneHotEncoder(sparse=False)\n",
    "display(pd.concat([X_train_transf,pd.DataFrame(ohe.fit_transform(km_fit.labels_.reshape(-1, 1)),\n",
    "columns=['cluster'+str(i) for i in range(0,km_fit.labels_.max()+1)],\n",
    "                    index=X_train_transf.index)],axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid search with cross-validation to find the best number of clusters\n",
    "param_grid = {'kmeans__n_clusters': range(2, 15)}  # specify the range of cluster numbers to try\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train_transf, y_train)\n",
    "\n",
    "C=[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "scor=[]\n",
    "for k in range(2,15):\n",
    "    km=KMeans(n_clusters=k,n_init=10)\n",
    "    km_fit=km.fit(X_train_transf)\n",
    "    ohe=OneHotEncoder(sparse=False)\n",
    "    X_cluster=pd.concat([X_train_transf,pd.DataFrame(ohe.fit_transform(km_fit.labels_.reshape(-1, 1)),\n",
    "    columns=['cluster'+str(i) for i in range(0,km_fit.labels_.max()+1)],\n",
    "                    index=X_train_transf.index)],axis=1)\n",
    "    for i in C:\n",
    "        logistic_regression = LogisticRegression(C=i)\n",
    "        scor.append([k,i,cross_validate(logistic_regression, X_cluster, y_train, \n",
    "                           cv=5,scoring=['f1','recall'])['test_recall'].mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(scor)[np.array(scor)[:,2].argmax(),:]\n",
    "\n",
    "k_cv=np.array(scor)[np.array(scor)[:,2].argmax(),0].astype(int)\n",
    "C_cv=np.array(scor)[np.array(scor)[:,2].argmax(),1]\n",
    "display(C_cv)\n",
    "\n",
    "print('The best hyperparameters and f1 score are [k C f1]:'+str(np.array(scor)[np.array(scor)[:,2].argmax(),:]))\n",
    "\n",
    "km=KMeans(n_clusters=k_cv,n_init=10)\n",
    "km_fit=km.fit(X_train_transf)\n",
    "ohe=OneHotEncoder(sparse=False)\n",
    "X_cluster=pd.concat([X_train_transf,pd.DataFrame(ohe.fit_transform(km_fit.labels_.reshape(-1, 1)),\n",
    "columns=['cluster'+str(i) for i in range(0,km_fit.labels_.max()+1)],\n",
    "                    index=X_train_transf.index)],axis=1)\n",
    "display(X_cluster.head(5))\n",
    "\n",
    "cluster=km_fit.predict(X_train_transf)\n",
    "#cluster=km_fit.predict(X_train_transf)\n",
    "unique, counts = np.unique(cluster, return_counts=True)\n",
    "display(pd.DataFrame(np.asarray((unique, counts)).T,columns=['cluster','no of obs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LogisticRegression()\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train_transf, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = grid_search.predict(X_test_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the logistic regression model\n",
    "#model = LinearSVC(loss='hinge',max_iter=10000)\n",
    "model = SVC(max_iter=10000)\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'kernel': ['linear', 'rbf','sigmoid']}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1',n_jobs=-1)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train_transf, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3 ,5, 8, 13, 21],\n",
    "    'min_samples_leaf': [ 6, 8],\n",
    "    'min_samples_split': [ 10, 20],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "tree_clf=DecisionTreeClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=tree_clf, param_grid=param_grid, cv=5,scoring='f1')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "tree_clf_cv=grid_search.fit(X_train_transf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "display(tree_clf_cv.best_score_)\n",
    "print(tree_clf_cv.best_params_)\n",
    "best_tree_params=tree_clf_cv.best_params_\n",
    "best_tree=DecisionTreeClassifier(**best_tree_params)\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plot_tree(best_tree.fit(X_train_transf, y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data=export_graphviz(best_tree,feature_names=X_train_transf.columns,\n",
    "                         class_names=['no default','default'],\n",
    "                         filled=True)\n",
    "\n",
    "graph=graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#não precisamos de adicionar hiperparametro devido ao facto de já termos a variancia normalizada das features\n",
    "#no limite, hiperparametros só no pré processamento, ajustadando no no de bins \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est=KBinsDiscretizer(n_bins=5,encode='ordinal',strategy='quantile')\n",
    "est.fit(X_train_tranf)\n",
    "Xt_train=est.transform(X_train_transf)\n",
    "Xt_test=est.transform(X_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnNB=MultinomialNB()\n",
    "mnNB.fit(Xt_train,y_train)\n",
    "y_pred=mnNB.predict(Xt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('discretizer', KBinsDiscretizer(encode='ordinal',strategy='quantile')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'discretizer__n_bins': [3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_transf, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "display(grid_search.best_estimator_)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "accuracy = best_model.score(X_test_transf, y_test)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gNB=GaussianNB()\n",
    "gNB.fit(X_train_transf,y_train)\n",
    "y_pred=gNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid={'n_estimators':[int(x) for x in np.linspace(10,100,4)],\n",
    "            'max_features':['auto','sqrt'],\n",
    "            'max_depth': [2, 3 ,5, 8, 13, 21],\n",
    "            'min_samples_leaf': [6, 8],\n",
    "            'min_samples_split': [10, 20],\n",
    "            'bootstrap':[True,False]\n",
    "}\n",
    "\n",
    "rf_clf=RandomForestClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "rf_clf_gs = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5,scoring='f1',n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "rf_clf_cv=rf_clf_gs.fit(X_train_transf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "display(rf_clf_cv.best_score_)\n",
    "best_rf_params=rf_clf_cv.best_params_\n",
    "display(best_rf_params)\n",
    "best_rf=RandomForestClassifier(**best_rf_params)\n",
    "best_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create the parameter grid\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "    'leaf_size': [10, 30, 50, 70]\n",
    "}\n",
    "\n",
    "# Create the k-NN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "knn_gs = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5,scoring='f1',n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "knn_clf_cv=knn_gs.fit(X_train_transf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "display(knn_clf_cv.best_score_)\n",
    "best_knn_params=knn_clf_cv.best_params_\n",
    "display(best_knn_params)\n",
    "best_knn=KNeighborsClassifier(**best_knn_params)\n",
    "best_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
      "[09:01:46] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameters for the XGBoost model\n",
    "xgbo_param_grid = {\n",
    "    'max_depth': [3 ,5, 8],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [50,100],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'reg_lambda': [ 0.5, 1]\n",
    "}\n",
    "\n",
    "# Create the k-NN classifier\n",
    "xgbo = xgb.XGBClassifier(n_jobs=-1,verbose=10)\n",
    "\n",
    "# Create the grid search object\n",
    "xgbo_gs = GridSearchCV(estimator=xgbo, param_grid=xgbo_param_grid, cv=5,scoring='f1',n_jobs=-1,verbose=10)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "xgbo_clf_cv=xgbo_gs.fit(X_train_transf, y_train,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7523584628308703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 0,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 100,\n",
       " 'reg_alpha': 0.5,\n",
       " 'reg_lambda': 1,\n",
       " 'subsample': 0.8}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best parameters\n",
    "display(xgbo_clf_cv.best_score_)\n",
    "best_xgbo_params=xgbo_clf_cv.best_params_\n",
    "display(best_xgbo_params)\n",
    "best_xgbo=xgb.XGBClassifier(**best_xgbo_params)\n",
    "best_xgbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "mlp_param_grid = {'hidden_layer_sizes': [(6,),(8,),(5,),(10,), (20,), (30,), (40,)],\n",
    "              'solver': ['adam', 'sgd'],\n",
    "              'activation': ['relu', 'tanh','logistic']}\n",
    "\n",
    "mlp=MLPClassifier(max_iter=2000)\n",
    "\n",
    "# Create the grid search object\n",
    "mlp_gs = GridSearchCV(estimator=mlp, param_grid=mlp_param_grid, cv=5,scoring='f1',n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "mlp_clf_cv=mlp_gs.fit(X_train_transf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "display(mlp_clf_cv.best_score_)\n",
    "best_mlp_params=mlp_clf_cv.best_params_\n",
    "display(best_mlp_params)\n",
    "best_mlp=MLPClassifier(**best_mlp_params)\n",
    "best_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinação de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit different models and evaluate their performance\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"{model.__class__.__name__}: {accuracy:.2f}\")\n",
    "    \n",
    "    #STACKING\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "# Define the second-level model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Define the stacking model\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = stacking_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = stacking_model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
