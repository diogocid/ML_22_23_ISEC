{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTE\n",
    "#All Libraries used\n",
    "#import pandas._libs.testing as _testing\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87500, 30)\n"
     ]
    }
   ],
   "source": [
    "#Import Dataset\n",
    "df = pd.read_csv('loan_default_prediction.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87500, 17)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop\n",
    "vdrop=['ID','Validation','Designation','Debt_to_Income','Postal_Code','Deprecatory_Records',\\\n",
    "            'Inquiries','Gross_Collection','Sub_GGGrade','Total_Unpaid_CL','File_Status','Claim_Type','Due_Fee']\n",
    "df=df.drop(vdrop,axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversão dos anos de experiência para numérico\n",
    "df['Experience']=df['Experience'].apply(lambda i: 10 if i[0:1]=='>' else 1 if i[0:1]=='<' else int(i[0:1]))\n",
    "#Conversão da duração para numérico\n",
    "df['Duration']=df['Duration'].apply(lambda i : i.replace(' years','years')).astype(str)\n",
    "#Conversão da GGGrade valor ordinal para numérico\n",
    "df['GGGrade']=df['GGGrade'].apply(romanToInt).astype(int)\n",
    "#criacao de debt to income\n",
    "#calcular o total da divida e o rendimento anual. uma espécie de \"taxa de esforço\"\n",
    "df['debt_to_income']=df['Unpaid_Amount']/df['Yearly_Income']\n",
    "#ver resultado\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77376, 18)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminar observações com pelo menos uma feature sem valores\n",
    "df=df.dropna()\n",
    "#drop duplicates\n",
    "df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asst_Reg</th>\n",
       "      <th>GGGrade</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Yearly_Income</th>\n",
       "      <th>Home_Status</th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>Lend_Amount</th>\n",
       "      <th>Interest_Charged</th>\n",
       "      <th>Usage_Rate</th>\n",
       "      <th>Present_Balance</th>\n",
       "      <th>State</th>\n",
       "      <th>Account_Open</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Unpaid_Amount</th>\n",
       "      <th>Reason</th>\n",
       "      <th>Default</th>\n",
       "      <th>debt_to_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>421802</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>633600.00</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42023.25</td>\n",
       "      <td>15.39</td>\n",
       "      <td>88.924</td>\n",
       "      <td>607161.90</td>\n",
       "      <td>California</td>\n",
       "      <td>17</td>\n",
       "      <td>3years</td>\n",
       "      <td>31216.05</td>\n",
       "      <td>debt  consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3964312</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>85483.20</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38133.00</td>\n",
       "      <td>9.94</td>\n",
       "      <td>102.856</td>\n",
       "      <td>269234.06</td>\n",
       "      <td>NC</td>\n",
       "      <td>15</td>\n",
       "      <td>5years</td>\n",
       "      <td>11660.49</td>\n",
       "      <td>debt  consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4247560</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>79200.00</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17100.00</td>\n",
       "      <td>22.35</td>\n",
       "      <td>60.372</td>\n",
       "      <td>22476.53</td>\n",
       "      <td>Florida</td>\n",
       "      <td>7</td>\n",
       "      <td>5years</td>\n",
       "      <td>5637.87</td>\n",
       "      <td>major  purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197179</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>61600.00</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5130.00</td>\n",
       "      <td>10.36</td>\n",
       "      <td>116.272</td>\n",
       "      <td>15242.09</td>\n",
       "      <td>NewJersey</td>\n",
       "      <td>9</td>\n",
       "      <td>3years</td>\n",
       "      <td>15607.17</td>\n",
       "      <td>major  purchase</td>\n",
       "      <td>1</td>\n",
       "      <td>0.253363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4646684</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>68053.92</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19665.00</td>\n",
       "      <td>13.68</td>\n",
       "      <td>127.280</td>\n",
       "      <td>65433.94</td>\n",
       "      <td>LA</td>\n",
       "      <td>10</td>\n",
       "      <td>5years</td>\n",
       "      <td>27472.86</td>\n",
       "      <td>debt  consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Asst_Reg  GGGrade  Experience  Yearly_Income Home_Status  Unpaid_2_years  \\\n",
       "0    421802        2          10      633600.00    MORTGAGE               0   \n",
       "1   3964312        4           7       85483.20        RENT               0   \n",
       "2   4247560        3           1       79200.00        RENT               0   \n",
       "3    197179        3           1       61600.00        RENT               0   \n",
       "4   4646684        5           2       68053.92        RENT               0   \n",
       "\n",
       "   Already_Defaulted  Lend_Amount  Interest_Charged  Usage_Rate  \\\n",
       "0                  0     42023.25             15.39      88.924   \n",
       "1                  0     38133.00              9.94     102.856   \n",
       "2                  0     17100.00             22.35      60.372   \n",
       "3                  0      5130.00             10.36     116.272   \n",
       "4                  0     19665.00             13.68     127.280   \n",
       "\n",
       "   Present_Balance       State  Account_Open Duration  Unpaid_Amount  \\\n",
       "0        607161.90  California            17   3years       31216.05   \n",
       "1        269234.06          NC            15   5years       11660.49   \n",
       "2         22476.53     Florida             7   5years        5637.87   \n",
       "3         15242.09   NewJersey             9   3years       15607.17   \n",
       "4         65433.94          LA            10   5years       27472.86   \n",
       "\n",
       "                Reason  Default  debt_to_income  \n",
       "0  debt  consolidation        0        0.049268  \n",
       "1  debt  consolidation        0        0.136407  \n",
       "2      major  purchase        0        0.071185  \n",
       "3      major  purchase        1        0.253363  \n",
       "4  debt  consolidation        0        0.403693  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.describe()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_num_cont=['Asst_Reg','Experience','Yearly_Income','Lend_Amount','Interest_Charged','Usage_Rate',\n",
    "            'Present_Balance','Unpaid_Amount','debt_to_income']\n",
    "v_num_disc=['Unpaid_2_years','Already_Defaulted','Account_Open']\n",
    "v_cat_ord=['Home_Status','State','Reason','Duration']#,'GGGrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MORTGAGE    39077\n",
      "RENT        30853\n",
      "OWN          7436\n",
      "OTHER           6\n",
      "NONE            4\n",
      "Name: Home_Status, dtype: int64\n",
      "California    11194\n",
      "Newyork        6414\n",
      "TX             6307\n",
      "Florida        5149\n",
      "IL             3091\n",
      "NewJersey      2877\n",
      "PA             2797\n",
      "Ohio           2602\n",
      "GA             2572\n",
      "VA             2251\n",
      "NC             2230\n",
      "MI             1995\n",
      "Maryland       1857\n",
      "AZ             1797\n",
      "MA             1764\n",
      "CO             1685\n",
      "WA             1627\n",
      "MN             1493\n",
      "IN             1276\n",
      "MO             1253\n",
      "TN             1184\n",
      "CT             1172\n",
      "NV             1039\n",
      "AL              999\n",
      "WI              990\n",
      "OR              929\n",
      "LA              908\n",
      "SC              888\n",
      "KY              728\n",
      "KS              722\n",
      "OK              676\n",
      "AR              564\n",
      "UT              556\n",
      "NM              424\n",
      "HI              423\n",
      "MS              370\n",
      "NH              365\n",
      "WV              344\n",
      "RI              337\n",
      "MT              225\n",
      "DC              206\n",
      "DE              205\n",
      "AK              198\n",
      "WY              167\n",
      "SD              162\n",
      "VT              155\n",
      "NE              120\n",
      "ND               45\n",
      "ME               44\n",
      "Name: State, dtype: int64\n",
      "debt  consolidation    46471\n",
      "credit  card           18626\n",
      "home  improvement       4326\n",
      "other                   3366\n",
      "major  purchase         1338\n",
      "medical                  717\n",
      "small  business          675\n",
      "car                      622\n",
      "moving                   446\n",
      "vacation                 367\n",
      "house                    290\n",
      "wedding                   96\n",
      "RENTwable  energy         36\n",
      "Name: Reason, dtype: int64\n",
      "3years    53111\n",
      "5years    24265\n",
      "Name: Duration, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#fazer histograma das categoricas e value_counts (verificar se há categorias de pouca relevancia)\n",
    "for i in v_cat_ord:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77376, 18)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(74542, 18)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "display(df.shape)\n",
    "#remover observações com home status 'none' e 'other'\n",
    "df=df[(df['Home_Status']!='OTHER')&(df['Home_Status']!='NONE')]\n",
    "#remover linhas com outliers, definidos como mais que 3 desvios-padrão acima/abaixo da média\n",
    "df=df[(np.abs(stats.zscore(df[v_num_cont])) < 3).all(axis=1)]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 18)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ver quantas observações têm default=1\n",
    "display((df['Default']==1).sum())\n",
    "#sample de 5000 obs com default =1\n",
    "defaulted = df[df['Default']==1].sample(n=5000, random_state=101)\n",
    "#sample de 5000 obs com default =0\n",
    "notdefault = df[df['Default']==0].sample(n=5000, random_state=101)\n",
    "#agregar as observações\n",
    "df = pd.concat([defaulted,notdefault],axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort aleatório das obs\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "#display(df.head(10))\n",
    "#display(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unpaid_2_years</th>\n",
       "      <th>Already_Defaulted</th>\n",
       "      <th>Account_Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>12.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.855792</td>\n",
       "      <td>0.076632</td>\n",
       "      <td>5.129104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unpaid_2_years  Already_Defaulted  Account_Open\n",
       "count    10000.000000       10000.000000  10000.000000\n",
       "mean         0.315900           0.005300     12.526900\n",
       "std          0.855792           0.076632      5.129104\n",
       "min          0.000000           0.000000      2.000000\n",
       "25%          0.000000           0.000000      9.000000\n",
       "50%          0.000000           0.000000     12.000000\n",
       "75%          0.000000           0.000000     15.000000\n",
       "max         16.000000           2.000000     51.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[v_num_disc].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MORTGAGE    4985\n",
      "RENT        4096\n",
      "OWN          919\n",
      "Name: Home_Status, dtype: int64\n",
      "California    1526\n",
      "TX             811\n",
      "Newyork        775\n",
      "Florida        651\n",
      "IL             391\n",
      "NewJersey      366\n",
      "PA             365\n",
      "Ohio           344\n",
      "GA             344\n",
      "NC             312\n",
      "MI             274\n",
      "VA             265\n",
      "Maryland       250\n",
      "AZ             245\n",
      "WA             226\n",
      "CO             226\n",
      "MA             216\n",
      "MN             199\n",
      "TN             151\n",
      "MO             149\n",
      "NV             143\n",
      "IN             142\n",
      "OR             139\n",
      "SC             133\n",
      "AL             128\n",
      "WI             120\n",
      "CT             119\n",
      "LA             114\n",
      "AR              88\n",
      "KS              88\n",
      "OK              81\n",
      "KY              79\n",
      "UT              73\n",
      "HI              60\n",
      "RI              52\n",
      "NM              52\n",
      "WV              43\n",
      "DC              40\n",
      "NH              37\n",
      "MS              31\n",
      "MT              28\n",
      "AK              28\n",
      "DE              24\n",
      "WY              19\n",
      "SD              18\n",
      "NE              14\n",
      "VT              11\n",
      "ND               5\n",
      "ME               5\n",
      "Name: State, dtype: int64\n",
      "debt  consolidation    6007\n",
      "credit  card           2389\n",
      "home  improvement       530\n",
      "other                   427\n",
      "major  purchase         178\n",
      "medical                 103\n",
      "car                      89\n",
      "small  business          87\n",
      "moving                   60\n",
      "vacation                 56\n",
      "house                    39\n",
      "wedding                  23\n",
      "RENTwable  energy        12\n",
      "Name: Reason, dtype: int64\n",
      "3years    7234\n",
      "5years    2766\n",
      "Name: Duration, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#fazer histograma das categoricas e value_counts (verificar se há categorias de pouca relevancia)\n",
    "for i in v_cat_ord:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train vs test sample: standard and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "#QUANDO PIPELINE ESTIVER PARA TODAS AS VARIÁVEIS, MUDAR O X\n",
    "X = df[v_num_cont]\n",
    "X = df[v_num_cont+v_num_disc+v_cat_ord]\n",
    "y = df['Default']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#script não ativado para fazer cross validation manual (normalmente o scikit learn faz tudo automático)\n",
    "from sklearn.model_selection import KFold\n",
    "#kfold = KFold(n_splits=5,shuffle=True)\n",
    "#for train_index, test_index in kfold.split(X):\n",
    "#    print(\"Train index:\", train_index, \"Test index:\", test_index)\n",
    "#    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "#    y_train, y_test = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    from utilities import skew_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#1a parte: tentar reduzir assimetria das variáveis\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#criar função que devolve uma métrica de assimetria das variáveis e avalia se essa variável\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#verificar assimetria das features numéricas\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m dskew\u001b[38;5;241m=\u001b[39m\u001b[43mskew_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv_num_cont\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mv_num_disc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#mostrar output\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#ver lista de features assimétricas, pvalue < 5%\u001b[39;00m\n\u001b[0;32m     13\u001b[0m v_skew\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(dskew\u001b[38;5;241m.\u001b[39mcolumns[dskew\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.05\u001b[39m])\n",
      "File \u001b[1;32m~\\ML-Meta\\utilities.py:24\u001b[0m, in \u001b[0;36mskew_df\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m skewtest\n\u001b[0;32m     23\u001b[0m skewness, p_value \u001b[38;5;241m=\u001b[39m skewtest(df)\n\u001b[1;32m---> 24\u001b[0m dskew\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mvstack((skewness\u001b[38;5;241m.\u001b[39mT,p_value\u001b[38;5;241m.\u001b[39mT)),\u001b[38;5;241m2\u001b[39m),columns\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m     25\u001b[0m                 index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskewness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(dskew)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#1a parte: tentar reduzir assimetria das variáveis\n",
    "\n",
    "#criar função que devolve uma métrica de assimetria das variáveis e avalia se essa variável\n",
    "#está suficientemente distante de 0 para ter certeza que é assimétrica\n",
    "#como se vê isso? quanto mais pequeno for o p-value (2a linha dataframe), mais certezas temos que é assimétrica\n",
    "\n",
    "#FUNÇÂO IMPORTADA UTILITIES  \n",
    "\n",
    "#verificar assimetria das features numéricas\n",
    "dskew=skew_df(X_train[v_num_cont+v_num_disc])\n",
    "#mostrar output\n",
    "#ver lista de features assimétricas, pvalue < 5%\n",
    "v_skew=list(dskew.columns[dskew.loc['p_value']<0.05])\n",
    "#lista de simétricas são as restantes\n",
    "v_sym=list(set(X_train[v_num_cont+v_num_disc].columns) - set(v_skew))\n",
    "#check\n",
    "v_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v_skew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Define the transformations to apply to the column\u001b[39;00m\n\u001b[0;32m      9\u001b[0m transformer \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m---> 10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myeoj\u001b[39m\u001b[38;5;124m'\u001b[39m, PowerTransformer(), \u001b[43mv_skew\u001b[49m), \u001b[38;5;66;03m#aplico transformação que corrige assimetria às assimétricas\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(), v_sym),     \u001b[38;5;66;03m#aplico transformação às simétricas (sub média e dividir desvio padrao)7\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moneh\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(min_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;66;03m#drop='if_binary',\u001b[39;00m\n\u001b[0;32m     13\u001b[0m                            sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), v_cat_ord)\n\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#pipeline= Pipeline([\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#('ct', transformer),\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#('to_df', pd.DataFrame, {'columns': v_skew+v_sym})\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Transform the data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#pfit = pipeline.fit(X_train)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m pfit \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'v_skew' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the transformations to apply to the column\n",
    "transformer = ColumnTransformer([\n",
    "    ('yeoj', PowerTransformer(), v_skew), #aplico transformação que corrige assimetria às assimétricas\n",
    "    ('std', StandardScaler(), v_sym),     #aplico transformação às simétricas (sub média e dividir desvio padrao)7\n",
    "    ('oneh', OneHotEncoder(min_frequency=0.05,handle_unknown='ignore',#drop='if_binary',\n",
    "                           sparse_output=False), v_cat_ord)\n",
    "])\n",
    "\n",
    "#pipeline= Pipeline([\n",
    "    #('ct', transformer),\n",
    "    #('to_df', pd.DataFrame, {'columns': v_skew+v_sym})\n",
    "    #(\"pandarizer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = (v_skew + v_sym))))\n",
    "#])\n",
    "\n",
    "# Transform the data\n",
    "#pfit = pipeline.fit(X_train)\n",
    "pfit = transformer.fit(X_train)\n",
    "\n",
    "categories= pfit.transformers_[2][1].categories_\n",
    "categories_out=pfit.transformers_[2][1].infrequent_categories_\n",
    "\n",
    "v_onehot_drop=list(np.concatenate([np.concatenate([categories[i][np.isin(categories[i], categories_out[i], invert=True)]],axis=0) \n",
    "                        for i in range(0,len(categories))],axis=0))\n",
    "#display(v_onehot_drop)\n",
    "\n",
    "v_onehot=list(np.concatenate([(np.concatenate((np.array(j),\n",
    "                       (np.array(['Other_Cat'+str(k)]) if categories_out[k] is not None else np.array([]))),axis=0)) \n",
    "                         for k,j in enumerate(\n",
    "                         [np.concatenate([categories[i][np.isin(categories[i], categories_out[i], invert=True)]],axis=0) \n",
    "                        for i in range(0,len(categories))]\n",
    "                         )]))\n",
    "#display(v_onehot)\n",
    "\n",
    "X_train_transf = transformer.transform(X_train)\n",
    "#X_train_transf = pipeline.transform(X_train)\n",
    "X_train_transf = pd.DataFrame(transformer.transform(X_train),columns = (v_skew + v_sym+v_onehot)\n",
    "                              ,index=X_train.index)\n",
    "#display(X_train_transf)\n",
    "#aplicar transformações ao teste calcula\n",
    "#X_test_transf = pipeline.transform(X_test)\n",
    "X_test_transf = transformer.transform(X_test)\n",
    "X_test_transf = pd.DataFrame(transformer.transform(X_test),columns = (v_skew + v_sym+v_onehot)\n",
    "                            ,index=X_test.index)\n",
    "\n",
    "#X_train_transformed= pd.DataFrame(df_transformed,\n",
    "                                  #columns=(v_skew+v_sym),index=X_train.index)\n",
    "\n",
    "display(round(X_train_transf.describe(),2))\n",
    "#vamos dar um check se o dataframe ficou \"menos assimétrico\"\n",
    "skew_df(X_train_transf[v_skew+v_sym])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fazer histograma das categoricas e value_counts (verificar se há categorias de pouca relevancia)\n",
    "cc=[]\n",
    "for i in v_cat_ord:\n",
    "    c=df[i].value_counts()/df.shape[0]\n",
    "    list(c[c>.05].index)\n",
    "    print(list(c[c>.05].index))\n",
    "    #cc=cc+c\n",
    "    \n",
    "cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply PCA to the training data to reduce the dimensionality\n",
    "#ALWAYS STANDARDIZE\n",
    "#VER SE VALE A PENA FAZER UNS GRAFICOS\n",
    "#POR EXEMPLO\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=0.9999)\n",
    "\n",
    "X_pca=pca.fit(X_train_transf[v_skew+v_sym])\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "#\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "#\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "#\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "#\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.80)\n",
    "\n",
    "X_pca=pca.fit(X_train_transf[v_skew+v_sym])\n",
    "\n",
    "X_train_pca=pd.DataFrame(pca.transform(X_train_transf[v_skew+v_sym]),\n",
    "                         columns=['pca_v'+str(i+1) for i in range (0,X_pca.n_components_)],\n",
    "                        index=X_train_transf.index)\n",
    "\n",
    "X_test_pca=pd.DataFrame(pca.transform(X_test_transf[v_skew+v_sym]),\n",
    "                         columns=['pca_v'+str(i+1) for i in range (0,X_pca.n_components_)],\n",
    "                        index=X_test_transf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_pca=pd.concat([X_train_pca,pd.DataFrame(y_train)],axis=1)\n",
    "#display(df_pca.head(5))\n",
    "\n",
    "sns.scatterplot(x='pca_v1', y='pca_v2', hue='Default', data=df_pca);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_pca.corr()\n",
    "display(round(corr.iloc[[-1],:],2))\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,cmap=\"BuPu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#############\n",
    "pipeline = Pipeline([\n",
    "(\"kmeans\", KMeans(n_clusters=10)),\n",
    "(\"log_reg\", LogisticRegression()),\n",
    "])\n",
    "#pipeline.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(kmeans__n_clusters=range(2, 50))\n",
    "display(param_grid)\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv=5)#, score='f1')\n",
    "grid_clf.fit(X_train_transf, y_train)\n",
    "\n",
    "display(grid_clf.best_params_)\n",
    "\n",
    "#grid_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_clf.best_params_)\n",
    "\n",
    "grid_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Apply K-means clustering to the training data\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "X_train_clusters = kmeans.fit_predict(X_train_transf)\n",
    "X_test_clusters = kmeans.predict(X_test_transf)\n",
    "\n",
    "# Create the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train_clusters.reshape(-1, 1), y_train)\n",
    "\n",
    "########################################\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Set up the cross-validation and grid search\n",
    "kfold = KFold(n_splits=5)\n",
    "param_grid = {'n_clusters': [2, 3, 4, 5, 6]}\n",
    "model = KMeans()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Re-train the model on the entire dataset with the best parameters\n",
    "model = KMeans(**grid_search.best_params_)\n",
    "X_clusters = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LogisticRegression()\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train_transf, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = grid_search.predict(X_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LinearSVC(loss='hinge',max_iter=10000)\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train_transf, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = grid_search.predict(X_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf=DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X_train_transf,y_train)\n",
    "y_pred=tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plot_tree(tree_clf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est=KBinsDiscretizer(n_bins=5,encode='ordinal',strategy='uniform')\n",
    "est.fit(X_train)\n",
    "Xt_train=est.transform(X_train_transf)\n",
    "Xt_test=est.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnNB=MultinomialNB()\n",
    "mnNB.fit(Xt_train,y_train)\n",
    "y_pred=mnNB.predict(Xt_test)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gNB=MultinomialNB()\n",
    "gNB.fit(X_train_transf,y_train)\n",
    "y_pred=gNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_Model=RandomForestClassifier()\n",
    "rf_Model.fit(X_train_transf,y_train)\n",
    "y_pred=rf_Model.predict(X_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of trees\n",
    "n_estimators=[int(x) for x in np.linspace(10,100,4)]\n",
    "#number of splits to consider at every split\n",
    "max_features=['auto','sqrt']\n",
    "#maximum number of levels\n",
    "max_depth=[2,4]\n",
    "#min of samples required to split a node\n",
    "min_samples_split=[2,5]\n",
    "#min samples required at each leaf node\n",
    "min_samples_leaf=[1,2]\n",
    "#method of selecting samples for training each tree\n",
    "bootstrap=[True,False]\n",
    "\n",
    "param_grid={'n_estimators':n_estimators,\n",
    "            'max_features':max_features,\n",
    "            'max_depth':max_depth,\n",
    "            'min_samples_split':min_samples_split,\n",
    "            'min_samples_leaf':min_samples_leaf,\n",
    "            'bootstrap':bootstrap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rf_Grid=GridSearchCV(rf_Model,param_grid,cv=5)\n",
    "rf_Grid.fit(X_train_transf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=rf_Grid.predict(X_test)\n",
    "rf_Grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_pred=knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # binary classification\n",
    "    'max_depth': 5,  # maximum depth of the tree\n",
    "    'learning_rate': 0.1,  # learning rate\n",
    "    'n_estimators': 100  # number of trees to be built\n",
    "}\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_transf, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test_transf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n",
    "##########################\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the data\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Set up the cross-validation and grid search\n",
    "kfold = KFold(n_splits=5)\n",
    "param_grid = {'learning_rate': [0.1, 0.2, 0.3],\n",
    "              'max_depth': [3, 4, 5],\n",
    "              'n_estimators': [100, 200, 300]}\n",
    "model = XGBClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Re-train the model on the entire dataset with the best parameters\n",
    "model = XGBClassifier(**grid_search.best_params_)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the test data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "##################################################### Set up the cross-validation and grid search\n",
    "kfold = KFold(n_splits=5)\n",
    "param_grid = {'hidden_layer_sizes': [(10,), (20,), (30,), (40,)],\n",
    "              'solver': ['adam', 'sgd'],\n",
    "              'activation': ['relu', 'tanh']}\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Re-train the model on the entire dataset with the best parameters\n",
    "model = MLPClassifier(max_iter=1000, **grid_search.best_params_)\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "###########################################################\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data\n",
    "X, y = load_data()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_shape=(X_train.shape[1],), activation='relu'),  # hidden layer\n",
    "    tf.keras.layers.Dense(y_train_one_hot.shape[1], activation='softmax')  # output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_one_hot = model.predict(X_test)\n",
    "\n",
    "# Convert the one-hot encoding back to labels\n",
    "y_pred = np.argmax(y_pred_one_hot, axis=1)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinação de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOTING\n",
    "\n",
    "#sklearn.ensemble.VotingClassifie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit different models and evaluate their performance\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"{model.__class__.__name__}: {accuracy:.2f}\")\n",
    "    \n",
    "    #STACKING\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "# Define the second-level model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Define the stacking model\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = stacking_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = stacking_model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "#print(accuracy_score(y_test,y_pred))\n",
    "#print(precision_score(y_test,y_pred))\n",
    "#print(recall_score(y_test,y_pred))\n",
    "#print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(cf_matrix)\n",
    "\n",
    "# Compute the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Generate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(precision_score(y_test,y_pred))\n",
    "print(recall_score(y_test,y_pred))\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,thresholds=roc_curve(y_test,y_pred)\n",
    "\n",
    "def plot_roc_curve(fpr,tpr):\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.plot([0,1],[0,1],'r')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC - TPR vs FPR')\n",
    "    \n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "y = [0, 1, 0, 1, 0]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Use Recursive Feature Elimination (RFE) to select the top 2 features\n",
    "rfe = RFE(model, 2)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and test data using the selected features\n",
    "X_train_selected = rfe.transform(X_train)\n",
    "X_test_selected = rfe.transform(X_test)\n",
    "\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the selected features of the test data\n",
    "predictions = model.predict(X_test_selected)\n",
    "\n",
    "# Print the accuracy\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    "############################################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "y = [0, 1, 0, 1, 0]\n",
    "\n",
    "# Create a list of all the feature indices\n",
    "feature_indices = list(range(X.shape[1]))\n",
    "\n",
    "# Set the initial set of features to be an empty list\n",
    "selected_features = []\n",
    "\n",
    "# Set the maximum number of features to select\n",
    "max_features = 2\n",
    "\n",
    "# Initialize the best cross-validated score to be negative infinity\n",
    "best_score = -np.inf\n",
    "\n",
    "# Iterate over all possible combinations of features\n",
    "for i in range(1, len(feature_indices) + 1):\n",
    "    for combination in combinations(feature_indices, i):\n",
    "        # Select the current combination of features\n",
    "        X_selected = X[:, combination]\n",
    "        \n",
    "        # Train a logistic regression model with 5-fold cross-validation\n",
    "        model = LogisticRegression()\n",
    "        score = cross_val_score(model, X_selected, y, cv=5).mean()\n",
    "        \n",
    "        # If the current combination of features has a higher cross-validated score than the best score, update the best score and the selected features\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            selected_features = combination\n",
    "\n",
    "# Print the selected features\n",
    "print(selected_features)\n",
    "##########################################\n",
    "#BACKWARD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "y = [0, 1, 0, 1, 0]\n",
    "\n",
    "# Create a list of all the feature indices\n",
    "feature_indices = list(range(X.shape[1]))\n",
    "\n",
    "# Set the initial set of features to be all the features\n",
    "selected_features = feature_indices\n",
    "\n",
    "# Set the minimum number of features to select\n",
    "min_features = 1\n",
    "\n",
    "# Initialize the best cross-validated score to be negative infinity\n",
    "best_score = -np.inf\n",
    "\n",
    "# Iterate over all possible combinations of features\n",
    "while len(selected_features) > min_features:\n",
    "    scores = []\n",
    "    for i in range(len(selected_features)):\n",
    "        # Select the current combination of features\n",
    "        X_selected = X[:, selected_features]\n",
    "        \n",
    "        # Train a logistic regression model with 5-fold cross-validation\n",
    "        model = LogisticRegression()\n",
    "        score = cross_val_score(model, X_selected, y, cv=5).mean()\n",
    "        \n",
    "        # Add the score for the current combination of features\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Find the index of the feature with the lowest score\n",
    "    worst_feature_index = np.argmin(scores)\n",
    "    \n",
    "    # If the current combination of features has a higher cross-validated score than the best score, update the best score and the selected features\n",
    "    if scores[worst_feature_index] > best_score:\n",
    "        best_score = scores[worst_feature_index]\n",
    "        selected_features = selected_features[:worst_feature_index] + selected_features[worst_feature_index + 1:]\n",
    "    else:\n",
    "        # If the current combination of features has a lower cross-validated score than the best score, stop the search\n",
    "        break\n",
    "\n",
    "# Print the selected features\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTERING AS PREPROCESSING DATA INTO CLASSIFICATION PROBLEM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "y = [0, 1, 0, 1, 0]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use KMeans to cluster the training data into 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Add the cluster labels as a new feature in the training data\n",
    "X_train_clustered = np.concatenate((X_train, kmeans.labels_.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# Create a Random Forest classifier and train it on the clustered data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_clustered, y_train)\n",
    "\n",
    "# Add the cluster labels as a new feature in the test data\n",
    "X_test_clustered = np.concatenate((X_test, kmeans.predict(X_test).reshape(-1, 1)), axis=1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test_clustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to pick the optimal number of clusters for a clustering algorithm is to use an evaluation metric that compares the quality of different clusterings. There are many such metrics available, such as the silhouette score, the Calinski-Harabasz index, and the Davies-Bouldin index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Try clustering the data with different numbers of clusters\n",
    "for n_clusters in range(2, 6):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    cluster_labels = kmeans.predict(X)\n",
    "\n",
    "    # Calculate the silhouette score for this number of clusters\n",
    "    score = silhouette_score(X, cluster_labels)\n",
    "    print(\"Number of clusters:\", n_clusters, \"Silhouette score:\", score)\n",
    "    \n",
    "    ###################\n",
    "#PROF DOC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "(\"kmeans\", KMeans(n_clusters=50)),\n",
    "(\"log_reg\", LogisticRegression()),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(kmeans__n_clusters=range(2, 100))\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "Let’s look at the best value for k and the performance of the resulting pipeline:\n",
    ">>> grid_clf.best_params_\n",
    "{'kmeans__n_clusters': 99}\n",
    ">>> grid_clf.score(X_test, y_test)\n",
    "0.9822222222222222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "\n",
    "# Create a PCA object and fit it to the data\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Plot the explained variance ratio as a function of the number of dimensions\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimal number of dimensions based on the explained variance ratio\n",
    "n_dimensions = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "\n",
    "# Transform the training and test data using the chosen number of dimensions\n",
    "X_train_transformed = pca.transform(X_train)[:, :n_dimensions]\n",
    "X_test_transformed = pca.transform(X_test)[:, :n_dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of clustering algorithms, but some of the most common ones include:\n",
    "\n",
    "    K-means clustering: This is a centroid-based algorithm that divides a dataset into a predefined number of clusters by minimizing the sum of squared distances between the points and the cluster centroids.\n",
    "\n",
    "    Hierarchical clustering: This is an agglomerative algorithm that builds a hierarchy of clusters by iteratively merging the closest pairs of clusters.\n",
    "\n",
    "    DBSCAN: This is a density-based algorithm that divides a dataset into clusters based on the density of the points. It is able to identify clusters of different shapes and sizes and can handle noisy or outlier points.\n",
    "\n",
    "    Expectation-maximization (EM): This is a probabilistic algorithm that estimates the underlying distribution of the data and uses it to identify clusters.\n",
    "\n",
    "    Affinity propagation: This is a message-passing algorithm that identifies clusters by exchanging messages between pairs of points until a consensus is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
